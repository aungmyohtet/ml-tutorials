24/8/2017
Studied KNN.
  Links followed
  1. https://saravananthirumuruganathan.wordpress.com/2010/05/17/a-detailed-introduction-to-k-nearest-neighbor-knn-algorithm/

Studied Naive bayes classifier
  Links followed
  1. https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/

31/8/2017
Completed the link (third time read) http://neuralnetworksanddeeplearning.com/chap1.html
Also read the source code

4/9/2017
I am close to fully understand backpropagation algorithm.
pdf in in my books-to-read folder in onedrive
close to understand raw python implementation of feed-forward network (https://github.com/mnielsen/neural-networks-and-deep-learning)

12/9/2017
Read the following link(s)
https://medium.com/@akashg/character-recognition-using-tensorflow-a93dbbdf4af

19/9/2017
Read the following link(s)
http://neuralnetworksanddeeplearning.com/chap3.html#the_cross-entropy_cost_function
Nots from this link
  Properties of cost function
    1. Must be positive
    2. Approach to zero when target output is close to actual output
  Some problems of quadratic cost function
    Learning can be slow when the error is big
  How to solve
    Use cross-entropy function

  The cross-entropy is nearly always the better choice

  Softmax layer and softmax function
  To continue reading the remaining sections